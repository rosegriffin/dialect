{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1628f0c1-f6ed-4c34-8a82-61d8bd774347",
   "metadata": {},
   "source": [
    "# American Dialect Classification with Extreme Learning Machines\n",
    "\n",
    "This notebook is my implementation of the approach described in *\"Word Based Dialect Classification using Extreme Learning Machines\"* by Rizwan et al. (2016).\n",
    "\n",
    "The acoustic characteristics that define American dialects have been a long standing interest of mine, and building a classifier seemed like a fun way to explore this. However, I struggled with the limitations that TIMIT presented and wasnâ€™t able to make much progress with my models. To overcome this, I decided to work through this paper as a learning exercise; to deepen my understanding of the techniques the authors used, and to serve as a useful reference point for experimenting with other approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2798e-ff10-41ca-9796-910b5439cef3",
   "metadata": {},
   "source": [
    "| Region # | Region name              | # Speakers |\n",
    "|----------|--------------------------|------------|\n",
    "| DR1      | New England              | 49         |\n",
    "| DR2      | Northern                 | 102        |\n",
    "| DR3      | North Midland            | 102        |\n",
    "| DR4      | South Midland            | 100        |\n",
    "| DR5      | Southern                 | 98         |\n",
    "| DR6      | New York City            | 46         |\n",
    "| DR7      | Western                  | 100        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2967a6-1d78-474a-93de-1cd8007ebec6",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545bf713-cd63-4595-ac68-866631fe5893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/words/TRAIN/DR1/FETB0/FETB0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/MMRP0/MMRP0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/FMEM0/FMEM0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/FSMA0/FSMA0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/FJSP0/FJSP0_SA1_0003_dark.wav']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "DATA_PATH = \"../data/processed/words\"\n",
    "N_REGIONS = 7 # ignore army brat\n",
    "\n",
    "train_files = {}\n",
    "test_files = {}\n",
    "words = [\"dark\", \"water\", \"greasy\", \"suit\", \"wash\"]\n",
    "\n",
    "# create dicts for the training and test set \n",
    "# where the key is a word, value is a list of lists\n",
    "# each element of the list are the word files for each dialect\n",
    "for word in words:\n",
    "    train_files[word] = []\n",
    "    test_files[word] = []\n",
    "    for i in range(1, N_REGIONS + 1):\n",
    "        train_list = glob.glob(f'{DATA_PATH}/TRAIN/DR{i}/*/*_{word}.wav')\n",
    "        test_list = glob.glob(f'{DATA_PATH}/TEST/DR{i}/*/*_{word}.wav')\n",
    "\n",
    "        train_files[word].append(train_list)\n",
    "        test_files[word].append(test_list)\n",
    "\n",
    "# first 5 samples of \"dark\" for DR1\n",
    "train_files[\"dark\"][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a31e3ba2-8706-4855-a1d1-c20221628932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "n_mfcc = 12\n",
    "sr = 16000\n",
    "n_fft = int(0.025 * sr)      # 25 ms\n",
    "hop_length = int(0.01 * sr)  # 10 ms\n",
    "win_length = n_fft\n",
    "\n",
    "def extract_features(wav):\n",
    "    \"\"\"\n",
    "    Extract MFCC + log-energy + delta + delta-delta\n",
    "    Output shape: (39, T)\n",
    "    \"\"\"\n",
    "    \n",
    "    y, _ = librosa.load(wav, sr=sr)\n",
    "\n",
    "    # peak normalization\n",
    "    peak = np.max(np.abs(y))\n",
    "    y = y / peak if peak > 0 else y\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=y,\n",
    "                                sr=sr,\n",
    "                                n_mfcc=n_mfcc,\n",
    "                                n_mels=26,\n",
    "                                n_fft=n_fft,\n",
    "                                hop_length=hop_length,\n",
    "                                win_length=win_length,\n",
    "                                window=\"hamming\",\n",
    "                                norm=None) # (12, T)\n",
    "\n",
    "    # log energy\n",
    "    S = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    energy = np.sum(abs(S) ** 2, axis=0)\n",
    "    log_energy = np.log(energy + 1e-8)\n",
    "\n",
    "    # concatenate to get 13 cepstral features\n",
    "    mfcc = np.vstack([mfcc, log_energy])\n",
    "\n",
    "    delta = librosa.feature.delta(mfcc, width=5, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, width=5, order=2)\n",
    "\n",
    "    # concatenate to get (39, T)\n",
    "    features = np.vstack([mfcc, delta, delta2])\n",
    "\n",
    "    # cmvn\n",
    "    mean = features.mean(axis=1, keepdims=True)\n",
    "    std = features.std(axis=1, keepdims=True) + 1e-8\n",
    "    features = (features - mean) / std\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_word_features(files):\n",
    "    \"\"\"\n",
    "    files[word][dialect] = list of wav files\n",
    "\n",
    "    Returns:\n",
    "        dict[word][dialect] = list of arrays, one per word sample.\n",
    "        Each array has shape (T_i, 39)\n",
    "    \"\"\"\n",
    "    all_features = {}\n",
    "\n",
    "    for word in files:\n",
    "        all_features[word] = []\n",
    "\n",
    "        for wavs_per_dialect in files[word]:\n",
    "            dialect_features = []\n",
    "\n",
    "            for wav in wavs_per_dialect:\n",
    "\n",
    "                features = extract_features(wav)   # (39, T)\n",
    "                frames = features.T                # (T, 39)\n",
    "\n",
    "                dialect_features.append(frames)\n",
    "\n",
    "            all_features[word].append(dialect_features)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "train_features = get_word_features(train_files)\n",
    "test_features = get_word_features(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df6aa057-dfd8-4322-b3c3-75693fa7e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "(33, 39)\n"
     ]
    }
   ],
   "source": [
    "# len should be n speakers for dialect\n",
    "print(len(train_features[\"dark\"][0]))\n",
    "\n",
    "# shape should be (n frames, 39)\n",
    "print(train_features[\"dark\"][0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf5a02-7b3d-4c08-b39b-0cdef2cd79c9",
   "metadata": {},
   "source": [
    "### Single Model Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9056178-8a0d-43b9-9cf7-bcc5190d8a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame level accuracy: 65.30%\n",
      "Error: 0.4542728329086879\n"
     ]
    }
   ],
   "source": [
    "from hpelm import ELM\n",
    "import os\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_features = (n_mfcc + 1) * 3 # 39\n",
    "\n",
    "def append_class_samples(features_1, features_2):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for sample in features_1:\n",
    "        X.append(sample)\n",
    "        y.append(np.full(sample.shape[0], 0))\n",
    "\n",
    "    for sample in features_2:\n",
    "        X.append(sample)\n",
    "        y.append(np.full(sample.shape[0], 1))\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def onehot(y, num_classes=2):\n",
    "    y_onehot = y_onehot = np.zeros((y.size, num_classes))\n",
    "    y_onehot[np.arange(y.size), y] = 1\n",
    "    \n",
    "    return y_onehot\n",
    "\n",
    "def get_weights(class_1, class_2):\n",
    "    \"\"\" Inverse class frequency \"\"\"\n",
    "    n1 = sum(sample.shape[0] for sample in class_1)\n",
    "    n2 = sum(sample.shape[0] for sample in class_2)\n",
    "    total = n1 + n2\n",
    "    return np.array([total / (2 * n1), total / (2 * n2)])\n",
    "\n",
    "class_1 = train_features[\"dark\"][0] # list of all class 1 samples, each shape (T, 39)\n",
    "class_2 = train_features[\"dark\"][1]\n",
    "weights = get_weights(class_1, class_2)\n",
    "\n",
    "X_train, y_train = append_class_samples(class_1, class_2)\n",
    "X_test, y_test = append_class_samples(test_features[\"dark\"][0],\n",
    "                                      test_features[\"dark\"][1])\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train\n",
    "elm_d1_d2 = ELM(n_features, 2)\n",
    "elm_d1_d2.add_neurons(500, \"sigm\")\n",
    "with open(os.devnull, \"w\") as f, redirect_stdout(f):\n",
    "    error = elm_d1_d2.train(X_train, onehot(y_train), 'CV', 'wc', w=weights, k=10)\n",
    "\n",
    "# eval\n",
    "y_pred = elm_d1_d2.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "\n",
    "print(f\"Frame level accuracy: {accuracy * 100:.2f}%\\nError: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097d46e-fd26-462b-a0bc-5201f259b760",
   "metadata": {},
   "source": [
    "### Single Word Pairwise ELM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d1de9cdd-6ded-4e7c-883f-49574e19b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pairwise_elms(word, log=False):\n",
    "    pairwise_elms = {}\n",
    "\n",
    "    for i in range(N_REGIONS):\n",
    "        for j in range(i + 1, N_REGIONS):\n",
    "\n",
    "            elm_name = f'DR{i + 1}, DR{j + 1}'\n",
    "\n",
    "            class_1_features = train_features[word][i] # 0\n",
    "            class_2_features = train_features[word][j] # 1\n",
    "            \n",
    "            X, y = append_class_samples(class_1_features, class_2_features)\n",
    "\n",
    "            # handle class imbalance \n",
    "            weights = get_weights(class_1_features, class_2_features)\n",
    "\n",
    "            # scale\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            y_oh = onehot(y)\n",
    "\n",
    "            elm = ELM(n_features, 2)\n",
    "            elm.add_neurons(1000, \"sigm\")\n",
    "            with open(os.devnull, \"w\") as f, redirect_stdout(f):\n",
    "                error = elm.train(X_scaled, y_oh, 'CV', 'wc', w=weights, k=10)\n",
    "\n",
    "            pairwise_elms[elm_name] = (elm, scaler, i, j)\n",
    "\n",
    "            if log:\n",
    "                print(f'{elm_name} : {error}')\n",
    "\n",
    "    return pairwise_elms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46c30d5f-837d-4528-8a7e-9a7ad8fe404a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_to_score(count):\n",
    "    \"\"\"\n",
    "    Assumes class will have 1 - 6 votes.\n",
    "    \"\"\"\n",
    "    mapping = {6 : 2 ** 1,\n",
    "               5 : 2 ** 0,\n",
    "               4 : 2 ** -1,\n",
    "               3 : 2 ** -2,\n",
    "               2 : 2 ** -3,\n",
    "               1 : 2 ** -4,\n",
    "               0 : 0}\n",
    "\n",
    "    return np.array(list(map(lambda x: mapping[x], count)))\n",
    "\n",
    "\n",
    "def classify_sample(X, pairwise_elms):\n",
    "    \"\"\"\n",
    "    Tallies binary classifier votes for a word sample.\n",
    "    Selects class with the highest score.\n",
    "    \"\"\"\n",
    "    count = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    for elm, scaler, class_1, class_2 in pairwise_elms.values():\n",
    "\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        y_pred = elm.predict(X_scaled)\n",
    "        y_pred = np.argmax(y_pred, axis=1) # frame level predictions\n",
    "        y_pred_label = np.bincount(y_pred).argmax() # overall prediction\n",
    "\n",
    "        count[(class_1, class_2)[y_pred_label]] += 1\n",
    "\n",
    "    scores = count_to_score(count)\n",
    "    \n",
    "    return np.argmax(scores)\n",
    "\n",
    "def eval_word(test_features, pairwise_elms):\n",
    "    decisions = []\n",
    "    \n",
    "    for i, dialect in enumerate(test_features):\n",
    "        for sample in dialect:\n",
    "            decision = classify_sample(sample, pairwise_elms)\n",
    "            decisions.append(1 if decision == i else 0)\n",
    "\n",
    "    decisions = np.array(decisions)\n",
    "    accuracy = np.mean(decisions)\n",
    "\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a34ea27-79bf-4689-90a0-2ddc309005e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR1, DR2 : 0.4062790112563349\n",
      "DR1, DR3 : 0.4050611536502899\n",
      "DR1, DR4 : 0.36183703462133837\n",
      "DR1, DR5 : 0.3806832387355392\n",
      "DR1, DR6 : 0.3861589507268691\n",
      "DR1, DR7 : 0.3946966015210175\n",
      "DR2, DR3 : 0.42102999833842986\n",
      "DR2, DR4 : 0.35621334859521564\n",
      "DR2, DR5 : 0.35060531435948705\n",
      "DR2, DR6 : 0.38500665612104135\n",
      "DR2, DR7 : 0.40619071135784657\n",
      "DR3, DR4 : 0.4161590602975426\n",
      "DR3, DR5 : 0.3784060665526682\n",
      "DR3, DR6 : 0.40339433196902763\n",
      "DR3, DR7 : 0.43583159716432196\n",
      "DR4, DR5 : 0.44817176256575125\n",
      "DR4, DR6 : 0.4045036042897037\n",
      "DR4, DR7 : 0.4007569525794503\n",
      "DR5, DR6 : 0.44814528019306776\n",
      "DR5, DR7 : 0.3578442984406571\n",
      "DR6, DR7 : 0.42576190976745865\n",
      "24.840764331210192\n"
     ]
    }
   ],
   "source": [
    "elms_dark = build_pairwise_elms(\"dark\", True)\n",
    "print(eval_word(test_features[\"dark\"], elms_dark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd6deea1-a9f9-4d86-9d7e-ec708292a74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 30.95541401273885 \n",
      "Std: 2.5027876056545857 \n",
      "Max: 35.6687898089172 \n",
      "Min: 26.751592356687897 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "\n",
    "for i in range(10):\n",
    "    elms = build_pairwise_elms(\"dark\")\n",
    "    accuracy = eval_word(test_features[\"dark\"], elms)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f'Mean: {np.mean(accuracies)} \\nStd: {np.std(accuracies)} \\nMax: {max(accuracies)} \\nMin: {min(accuracies)} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d2d96-10f6-4435-b54e-716fb69b5856",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Garofolo, John S., et al. TIMIT Acoustic-Phonetic Continuous Speech Corpus LDC93S1. Web \n",
    "Download. Philadelphia: Linguistic Data Consortium, 1993.\n",
    "\n",
    "2. Rizwan, M., Odelowo, B. O., & Anderson, D. V. (2016, July). Word based dialect classification using extreme learning machines. In 2016 International Joint Conference on Neural Networks (IJCNN) (pp. 2625-2629). IEEE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
