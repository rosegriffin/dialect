{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1628f0c1-f6ed-4c34-8a82-61d8bd774347",
   "metadata": {},
   "source": [
    "# American Dialect Classification with Extreme Learning Machines\n",
    "\n",
    "This notebook is my implementation of the approach described in *\"Word Based Dialect Classification using Extreme Learning Machines\"* by Rizwan et al. (2016).\n",
    "\n",
    "The acoustic characteristics that define American dialects have been a long standing interest of mine, and building a classifier seemed like a fun way to explore this. However, I struggled with the limitations that TIMIT presented and wasnâ€™t able to make much progress with my models. To overcome this, I decided to work through this paper as a learning exercise; to deepen my understanding of the techniques the authors used, and to serve as a useful reference point for experimenting with other models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2798e-ff10-41ca-9796-910b5439cef3",
   "metadata": {},
   "source": [
    "| Region # | Region name              | # Speakers |\n",
    "|----------|--------------------------|------------|\n",
    "| DR1      | New England              | 49         |\n",
    "| DR2      | Northern                 | 102        |\n",
    "| DR3      | North Midland            | 102        |\n",
    "| DR4      | South Midland            | 100        |\n",
    "| DR5      | Southern                 | 98         |\n",
    "| DR6      | New York City            | 46         |\n",
    "| DR7      | Western                  | 100        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2967a6-1d78-474a-93de-1cd8007ebec6",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545bf713-cd63-4595-ac68-866631fe5893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/words/TRAIN/DR1/FETB0/FETB0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/MMRP0/MMRP0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/FMEM0/FMEM0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/FSMA0/FSMA0_SA1_0003_dark.wav',\n",
       " '../data/processed/words/TRAIN/DR1/FJSP0/FJSP0_SA1_0003_dark.wav']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "DATA_PATH = \"../data/processed/words\"\n",
    "N_REGIONS = 7 # ignore army brat\n",
    "\n",
    "train_files = {}\n",
    "test_files = {}\n",
    "words = [\"dark\", \"water\", \"greasy\", \"suit\", \"wash\"]\n",
    "\n",
    "# create dicts for the training and test set \n",
    "# where the key is a word, value is a list of lists\n",
    "# each element of the list are the word files for each dialect\n",
    "for word in words:\n",
    "    train_files[word] = []\n",
    "    test_files[word] = []\n",
    "    for i in range(1, N_REGIONS + 1):\n",
    "        train_list = glob.glob(f'{DATA_PATH}/TRAIN/DR{i}/*/*_{word}.wav')\n",
    "        test_list = glob.glob(f'{DATA_PATH}/TEST/DR{i}/*/*_{word}.wav')\n",
    "\n",
    "        train_files[word].append(train_list)\n",
    "        test_files[word].append(test_list)\n",
    "\n",
    "# first 5 samples of \"dark\" for DR1\n",
    "train_files[\"dark\"][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a31e3ba2-8706-4855-a1d1-c20221628932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "n_mfcc = 12\n",
    "sr = 16000\n",
    "n_fft = int(0.025 * sr)      # 25 ms\n",
    "hop_length = int(0.01 * sr)  # 10 ms\n",
    "win_length = n_fft\n",
    "\n",
    "def extract_features(wav):\n",
    "    \"\"\"\n",
    "    Extract MFCC + log-energy + delta + delta-delta\n",
    "    Output shape: (39, T)\n",
    "    \"\"\"\n",
    "    \n",
    "    y, _ = librosa.load(wav, sr=sr)\n",
    "    y = librosa.util.normalize(y) # amplitude normalization\n",
    "    mfcc = librosa.feature.mfcc(y=y,\n",
    "                                sr=sr,\n",
    "                                n_mfcc=n_mfcc,\n",
    "                                n_fft=n_fft,\n",
    "                                hop_length=hop_length,\n",
    "                                win_length=win_length,\n",
    "                                window=\"hamming\",\n",
    "                                htk=True,\n",
    "                                center=False) # (12, T)\n",
    "\n",
    "    # log energy\n",
    "    frames = librosa.util.frame(y, frame_length=n_fft, hop_length=hop_length)\n",
    "    energy = np.sum(frames ** 2, axis=0)\n",
    "    energy = energy / np.max(energy + 1e-8) # normalize\n",
    "    log_energy = np.log(energy + 1e-10) # (T,)\n",
    "\n",
    "    # ensure frame alignment\n",
    "    T = min(mfcc.shape[1], log_energy.shape[0])\n",
    "    mfcc = mfcc[:, :T]\n",
    "    log_energy = log_energy[:T]\n",
    "\n",
    "    # concatenate to get 13 cepstral features\n",
    "    mfcc = np.vstack([mfcc, log_energy])\n",
    "\n",
    "    delta = librosa.feature.delta(mfcc, width=5, order=1)\n",
    "    delta2 = librosa.feature.delta(mfcc, width=5, order=2)\n",
    "\n",
    "    # concatenate to get (39, T)\n",
    "    features = np.vstack([mfcc, delta, delta2])\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_word_features(files):\n",
    "    \"\"\"\n",
    "    files: dict[word] = list of lists of wavs of word per dialect\n",
    "    returns: dict[word] = list of arrays, each array is one word sample (pooled)\n",
    "    \"\"\"\n",
    "    all_features = {}\n",
    "\n",
    "    for word in files:\n",
    "        all_features[word] = []\n",
    "\n",
    "        for wavs_per_dialect in files[word]:\n",
    "            dialect_word_instances = []\n",
    "\n",
    "            for wav in wavs_per_dialect:\n",
    "\n",
    "                features = extract_features(wav)   # (39, T)\n",
    "                frames = features.T                # (T, 39)\n",
    "\n",
    "                dialect_word_instances.append(frames)\n",
    "\n",
    "            all_features[word].append(dialect_word_instances)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "train_features = get_word_features(train_files)\n",
    "test_features = get_word_features(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6aa057-dfd8-4322-b3c3-75693fa7e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 39)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape should be (num_speakers_in_region, 39)\n",
    "train_features[\"dark\"][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf5a02-7b3d-4c08-b39b-0cdef2cd79c9",
   "metadata": {},
   "source": [
    "### Single Model Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9056178-8a0d-43b9-9cf7-bcc5190d8a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 67.12%\n",
      "Loss: 0.31942863232632646\n"
     ]
    }
   ],
   "source": [
    "from hpelm import ELM\n",
    "import os\n",
    "import sys\n",
    "from contextlib import redirect_stdout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_features = (n_mfcc + 1) * 3 # 39\n",
    "\n",
    "def append_class_samples(features_1, features_2):\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for sample in features_1:\n",
    "        X.append(sample)\n",
    "        y.append(np.full(sample.shape[0], 0))\n",
    "\n",
    "    for sample in features_2:\n",
    "        X.append(sample)\n",
    "        y.append(np.full(sample.shape[0], 1))\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.hstack(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def onehot(y, num_classes=2):\n",
    "    y_onehot = y_onehot = np.zeros((y.size, num_classes))\n",
    "    y_onehot[np.arange(y.size), y] = 1\n",
    "    \n",
    "    return y_onehot\n",
    "\n",
    "def get_weights(class_1, class_2):\n",
    "    return np.array([sum([sample.shape[0] for sample in class_1]), sum([sample.shape[0] for sample in class_2])])\n",
    "\n",
    "class_1 = train_features[\"dark\"][0] # list of all class 1 samples, shape (T, 39)\n",
    "class_2 = train_features[\"dark\"][1]\n",
    "weights = get_weights(class_1, class_2)\n",
    "\n",
    "X_train, y_train = append_class_samples(class_1, class_2)\n",
    "X_test, y_test = append_class_samples(test_features[\"dark\"][0],\n",
    "                                      test_features[\"dark\"][1])\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# train\n",
    "elm_d1_d2 = ELM(n_features, 2)\n",
    "elm_d1_d2.add_neurons(500, \"sigm\")\n",
    "with open(os.devnull, \"w\") as f, redirect_stdout(f):\n",
    "    loss = elm_d1_d2.train(X_train_scaled, onehot(y_train), 'CV', 'wc', w=weights, k=10)\n",
    "\n",
    "# eval\n",
    "y_pred = elm_d1_d2.predict(X_test_scaled)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\\nLoss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e097d46e-fd26-462b-a0bc-5201f259b760",
   "metadata": {},
   "source": [
    "### Single Word Pairwise ELM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1de9cdd-6ded-4e7c-883f-49574e19b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pairwise_elms(word, hidden_range=range(100, 1000 + 1, 100)):\n",
    "    pairwise_elms = {}\n",
    "\n",
    "    for i in range(N_REGIONS):\n",
    "        for j in range(i + 1, N_REGIONS):\n",
    "\n",
    "            elm_name = f'DR{i + 1}, DR{j + 1}'\n",
    "\n",
    "            class_1_features = train_features[word][i] # 0\n",
    "            class_2_features = train_features[word][j] # 1\n",
    "            \n",
    "            X, y = append_class_samples(class_1_features, class_2_features)\n",
    "\n",
    "            # handle class imbalance\n",
    "            weights = get_weights(class_1_features, class_2_features)\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            y_oh = onehot(y)\n",
    "\n",
    "            elm = ELM(n_features, 2)\n",
    "            elm.add_neurons(1000, \"sigm\")\n",
    "            with open(os.devnull, \"w\") as f, redirect_stdout(f):\n",
    "                error = elm.train(X_scaled, y_oh, 'CV', 'wc', w=weights, k=10)\n",
    "\n",
    "            pairwise_elms[elm_name] = (elm, scaler, i, j)\n",
    "            \n",
    "            print(f'{elm_name} : {error}%')\n",
    "\n",
    "    return pairwise_elms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a34ea27-79bf-4689-90a0-2ddc309005e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR1, DR2 : 0.3059641987302921%\n",
      "DR1, DR3 : 0.287922343770176%\n",
      "DR1, DR4 : 0.2943430300066737%\n",
      "DR1, DR5 : 0.2822035070320056%\n",
      "DR1, DR6 : 0.36451905746023616%\n",
      "DR1, DR7 : 0.30645509550893735%\n",
      "DR2, DR3 : 0.3938684746311273%\n",
      "DR2, DR4 : 0.34092383737179055%\n",
      "DR2, DR5 : 0.3129571123961613%\n",
      "DR2, DR6 : 0.3130293649142041%\n",
      "DR2, DR7 : 0.38058839348619355%\n",
      "DR3, DR4 : 0.3992642040476337%\n",
      "DR3, DR5 : 0.35593795769693526%\n",
      "DR3, DR6 : 0.3058620516719931%\n",
      "DR3, DR7 : 0.4075103217426428%\n",
      "DR4, DR5 : 0.4130840799821466%\n",
      "DR4, DR6 : 0.3251543298918204%\n",
      "DR4, DR7 : 0.37831161078397285%\n",
      "DR5, DR6 : 0.3359222416706203%\n",
      "DR5, DR7 : 0.32582889268451%\n",
      "DR6, DR7 : 0.3263431069645981%\n"
     ]
    }
   ],
   "source": [
    "elms_dark = build_pairwise_elms(\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46c30d5f-837d-4528-8a7e-9a7ad8fe404a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.210191082802545\n"
     ]
    }
   ],
   "source": [
    "def count_to_score(count):\n",
    "    \"\"\"\n",
    "    Assumes class will have 1 - 6 votes.\n",
    "    \"\"\"\n",
    "    mapping = {6 : 2 ** 1,\n",
    "               5 : 2 ** 0,\n",
    "               4 : 2 ** -1,\n",
    "               3 : 2 ** -2,\n",
    "               2 : 2 ** -3,\n",
    "               1 : 2 ** -4,\n",
    "               0 : 0}\n",
    "\n",
    "    return np.array(list(map(lambda x: mapping[x], count)))\n",
    "\n",
    "\n",
    "def classify_sample(X, pairwise_elms):\n",
    "    \"\"\"\n",
    "    Tallies binary classifier votes for a word sample.\n",
    "    Selects class with the highest score.\n",
    "    \"\"\"\n",
    "    count = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    for elm, scaler, class_1, class_2 in pairwise_elms.values():\n",
    "\n",
    "        X_scaled = scaler.transform(X)\n",
    "        y_pred = elm.predict(X_scaled)\n",
    "        y_pred = np.argmax(y_pred, axis=1) # frame level predictions\n",
    "        y_pred_label = np.bincount(y_pred).argmax() # overall prediction\n",
    "\n",
    "        count[(class_1, class_2)[y_pred_label]] += 1\n",
    "\n",
    "    scores = count_to_score(count)\n",
    "\n",
    "    return np.argmax(scores)\n",
    "\n",
    "def eval_word(test_features, pairwise_elms):\n",
    "    decisions = []\n",
    "    \n",
    "    for i, dialect in enumerate(test_features):\n",
    "        for sample in dialect:\n",
    "            decision = classify_sample(sample, pairwise_elms)\n",
    "            decisions.append(1 if decision == i else 0)\n",
    "\n",
    "    decisions = np.array(decisions)\n",
    "    accuracy = np.mean(decisions)\n",
    "\n",
    "    print(f'{accuracy * 100}')\n",
    "\n",
    "\n",
    "eval_word(test_features[\"dark\"], elms_dark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d2d96-10f6-4435-b54e-716fb69b5856",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Garofolo, John S., et al. TIMIT Acoustic-Phonetic Continuous Speech Corpus LDC93S1. Web \n",
    "Download. Philadelphia: Linguistic Data Consortium, 1993.\n",
    "\n",
    "2. Rizwan, M., Odelowo, B. O., & Anderson, D. V. (2016, July). Word based dialect classification using extreme learning machines. In 2016 International Joint Conference on Neural Networks (IJCNN) (pp. 2625-2629). IEEE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
